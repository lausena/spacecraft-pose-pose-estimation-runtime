{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95276a61-386d-4ebb-9ce4-f64a5e070b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/m/Documents/DNN Final Project/spacecraft-pose-pose-estimation-runtime/juptyer-notebooks\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b25795b-5f49-4e90-aa8a-3cb69b3bd5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 22:09:11.997610: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-24 22:09:11.997679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-24 22:09:12.012743: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-24 22:09:12.069174: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 22:09:13.978594: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.transform import Rotation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import keras.preprocessing.image as kimg\n",
    "from keras.preprocessing import image_dataset_from_directory\n",
    "from keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D, Concatenate, Flatten, Conv2D, MaxPooling2D, AvgPool2D\n",
    "from keras.applications import ResNet50, MobileNetV2\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model\n",
    "import math\n",
    "from math import cos, sin\n",
    "from autograd import jacobian\n",
    "import tensorflow as tf\n",
    "#import tensorflow_probability as tfp\n",
    "#tfd = tfp.distributions\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.utils import image_dataset_from_directory as idfd\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8214af42-fbe0-4b14-a782-5b01b5ce8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/m/Documents/DNN Final Project/spacecraft-pose-pose-estimation-runtime/juptyer-notebooks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b7a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMERA_MATRIX = np.array([[5.2125371e+03, 0.0000000e+00, 6.4000000e+02],\n",
    "                          [0.0000000e+00, 6.2550444e+03, 5.1200000e+02],\n",
    "                          [0.0000000e+00, 0.0000000e+00, 1.0000000e+00]])\n",
    "IMAGE_WIDTH = 1280\n",
    "IMAGE_HEIGHT = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c53b430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path, base_path):\n",
    "  raw = tf.io.read_file(path)\n",
    "  image = tf.image.resize(tf.image.decode_png(raw, channels=3), (512,640))\n",
    "  base_raw = tf.io.read_file(base_path)\n",
    "  base_image = tf.image.resize(tf.image.decode_png(base_raw, channels=3), (512,640))\n",
    "\n",
    "  return base_image, image\n",
    "\n",
    "\n",
    "#df_labels = dfY[[\"x\",\"y\",\"z\",\"qw\",\"qx\",\"qy\",\"qz\"]]\n",
    "#dfY.drop([\"chain_id\", \"i\", \"x\",\"y\",\"z\",\"qw\",\"qx\",\"qy\",\"qz\"], axis=1, inplace=True)\n",
    "#print(dfY)\n",
    "#dfY = np.asarray(dfY).astype(np.float32)\n",
    "#dfY.drop([\"chain_id\", \"i\"], axis=1, inplace=True)\n",
    "#print(dfY)\n",
    "#ds = tf.data.Dataset.from_tensor_slices((dfY, {\"x\": \"x\", \"y\": \"y\", \"z\": \"z\", \"qw\": \"qw\", \"qx\": \"qx\", \"qy\": \"qy\", \"qz\": \"qz\"}))\n",
    "\n",
    "# The tuples are unpacked into the positional arguments of the mapped function\n",
    "def load_and_preprocess_from_path_label(path, base_path, label):\n",
    "  return load_and_preprocess_image(path, base_path), label\n",
    "\n",
    "# train_images = x_train.apply(lambda x: load_and_preprocess_image(x[\"path\"],x[\"base_path\"]), axis=1)\n",
    "# test_images = x_test.apply(lambda x: load_and_preprocess_image(x[\"path\"],x[\"base_path\"]), axis=1)\n",
    "# # print(type(image_label_ds[0]))\n",
    "# # print(image_label_ds[:,0])\n",
    "# base_images = [x[0] for x in train_images]\n",
    "# images = [x[1] for x in train_images]\n",
    "# base_images_test = [x[0] for x in test_images]\n",
    "# images_test = [x[1] for x in test_images]\n",
    "# print(np.asarray(base_images).shape)\n",
    "# for elem in image_label_ds.take(1):\n",
    "#   print(elem[0][0].shape)\n",
    "#print(list(image_label_ds.as_numpy_iterator())[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07eac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_from_df(dataframe: pd.DataFrame, chunk_size: int = 10):\n",
    "    for start_row in range(0, dataframe.shape[0], chunk_size):\n",
    "        end_row  = min(start_row + chunk_size, dataframe.shape[0])\n",
    "        yield dataframe.iloc[start_row:end_row, :]\n",
    "get_chunk = flow_from_df(x_train, 100)\n",
    "\n",
    "def load_new_batch():\n",
    "    train_images = next(get_chunk).apply(lambda x: load_and_preprocess_image(x[\"path\"],x[\"base_path\"]), axis=1)\n",
    "    base_images = [x[0] for x in train_images]\n",
    "    images = [x[1] for x in train_images]\n",
    "    del train_images\n",
    "\n",
    "    return base_images, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4705b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfY = pd.read_csv(\"../data/train_labels.csv\")[:15000]\n",
    "dfY[\"path\"] = [f\"../data/images/{id}/{i:03d}.png\" for id, i in zip(dfY[\"chain_id\"], dfY[\"i\"])]\n",
    "dfY[\"base_path\"] = [f\"../data/images/{id}/000.png\" for id, i in zip(dfY[\"chain_id\"], dfY[\"i\"])]\n",
    "x_train, x_test, y_train, y_test = train_test_split(dfY[[\"path\", \"base_path\"]], dfY[[\"x\", \"y\", \"z\", \"qw\",\"qx\",\"qy\",\"qz\"]], test_size=0.95, random_state=42)\n",
    "x_train = pd.DataFrame(x_train, columns=[\"path\", \"base_path\"])\n",
    "x_test = pd.DataFrame(x_test, columns=[\"path\", \"base_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "267d0595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14250 images belonging to None classes\n",
      "285/285 [==============================] - 1229s 4s/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#ypred_rot = pd.DataFrame(model_rot.predict(CustomDataGenerator(x_test, y_test, 20)), index=y_test.index, columns=[\"qw\", \"qx\", \"qy\", \"qz\"])\n",
    "ypred = pd.DataFrame(model_pos.predict(CustomDataGenerator(x_test, y_test, 50)), index=y_test.index, columns=[\"x\", \"y\", \"z\", \"qw\", \"qx\", \"qy\", \"qz\"])\n",
    "#ypred = pd.merge(ypred_pos, ypred_rot, left_index=True, right_index=True)\n",
    "#ypred[[\"qw\", \"qx\", \"qy\", \"qz\"]] = ypred[[\"qw\", \"qx\", \"qy\", \"qz\"]].clip(lower=-1, upper=1)\n",
    "\n",
    "ypred[\"chain_id\"] = dfY.iloc[ypred.index].chain_id\n",
    "ypred[\"i\"] = dfY.iloc[ypred.index].i.astype(int)\n",
    "y_pred_norm = np.linalg.norm(ypred[[\"qw\", \"qx\", \"qy\", \"qz\"]], axis=1)\n",
    "ypred[\"qw\"] = ypred[\"qw\"] / y_pred_norm\n",
    "ypred[\"qx\"] = ypred[\"qx\"] / y_pred_norm\n",
    "ypred[\"qy\"] = ypred[\"qy\"] / y_pred_norm\n",
    "ypred[\"qz\"] = ypred[\"qz\"] / y_pred_norm\n",
    "#ypred.loc[ypred.i == 0, [\"x\", \"y\", \"z\", \"qw\", \"qx\", \"qy\", \"qz\"]] = [0,0,0,1,0,0,0]\n",
    "# ypred[[\"qw\", \"qx\", \"qy\", \"qz\"]] = [0,0,0,0]\n",
    "#ypred[[\"x\", \"y\", \"z\"]] = [0,0,0]\n",
    "#ypred = ypred[[\"chain_id\", \"i\", \"x\", \"y\", \"z\", \"qw\", \"qx\", \"qy\", \"qz\"]]\n",
    "ypred.loc[ypred.i == 0, [\"x\", \"y\", \"z\", \"qw\", \"qx\", \"qy\", \"qz\"]] = [0,0,0,1,0,0,0]\n",
    "ypred.set_index([\"chain_id\", \"i\"], inplace=True)\n",
    "ToCSV(ypred, \"cnn-test\")\n",
    "y_copy = y_test.copy()\n",
    "y_copy[\"chain_id\"] = dfY.iloc[y_copy.index].chain_id\n",
    "y_copy[\"i\"] = dfY.iloc[y_copy.index].i.astype(int)\n",
    "y_copy.set_index([\"chain_id\", \"i\"], inplace=True)\n",
    "ToCSV(y_copy, \"cnn-true\")\n",
    "\n",
    "#model.save('../models/test_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "431fd5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pos.save('../models/test_model-logcosh-16x3-3e-00001-larger-kernels.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c49f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv')\n",
    "\n",
    "# Loads all images found and trains given model\n",
    "def load_images_and_train_model(model, is_pos_model):\n",
    "    with tf.device('/GPU:0'):\n",
    "        model.summary()\n",
    "        \n",
    "        #for i in range(batches):\n",
    "            #batch = load_new_batch()\n",
    "            #model.fit([np.asarray(batch[0]), np.asarray(batch[1])], y_train[100 * i : 100*(i+1)], epochs=40, batch_size=batch_size_b)\n",
    "        if is_pos_model:\n",
    "            ypos = pd.DataFrame(y_train, index=y_train.index, columns=[\"x\", \"y\", \"z\"])\n",
    "            model.fit(CustomDataGenerator(x_train, y_train, batch_size=100), epochs=3)\n",
    "        else:\n",
    "            yquat = pd.DataFrame(y_train, index=y_train.index, columns=[\"qw\", \"qx\", \"qy\", \"qz\"])\n",
    "            model.fit(CustomDataGenerator(x_train, yquat, batch_size=100), epochs=3)\n",
    "\n",
    "\n",
    "        \n",
    "    #     ypreddf = pd.DataFrame(ypred.T, index=image_label_ds.take(count * 100 * 0.2).index_array, columns=[\"x\", \"y\", \"z\"])\n",
    "    #     dfY = pd.read_csv(\"../data/train_labels.csv\")[:count * 100]\n",
    "    #     dfY_merged = pd.merge(ypreddf.drop([\"x\", \"y\", \"z\"], axis=1), dfY, left_index=True, right_index=True)\n",
    "    #     dfYidx = dfY.drop([\"x\", \"y\", \"z\", \"qw\", \"qx\", \"qy\", \"qz\"], axis=1)\n",
    "    #     df_merged = pd.merge(ypreddf, dfYidx, left_index=True, right_index=True)\n",
    "    #     df_merged[\"qw\"] = 0\n",
    "    #     df_merged[\"qx\"] = 0\n",
    "    #     df_merged[\"qy\"] = 0\n",
    "    #     df_merged[\"qz\"] = 0\n",
    "    #     df_merged.loc[df_merged[\"i\"] == 0, [\"x\", \"y\", \"z\", \"qw\", \"qx\", \"qy\", \"qz\"]] = [0,0,0,1,0,0,0]\n",
    "    #     df_merged.set_index([\"chain_id\", \"i\"], inplace=True)\n",
    "    #    # df_merged.drop([\"id\"], axis=1, inplace=True)\n",
    "    #     ToCSV(df_merged, \"cnn-test\")\n",
    "    #     ToCSV(dfY_merged, \"cnn-true\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# from https://github.com/1988kramer/camera-pose/blob/master/camera-pose.py \n",
    "def create_conv_branch(input_shape):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(96, kernel_size=(11,11),\n",
    "\t\t\t\t\t strides=4, padding='valid',\n",
    "\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\t input_shape=input_shape))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\tmodel.add(Conv2D(128, kernel_size=(5,5),\n",
    "\t\t\t\t\t strides=1, padding='same',\n",
    "\t\t\t\t\t activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(3,3), strides=1))\n",
    "\tmodel.add(Conv2D(256, kernel_size=(3,3),\n",
    "\t\t\t\t\t strides=1, padding='same',\n",
    "\t\t\t\t\t activation='relu'))\n",
    "\tmodel.add(Conv2D(256, kernel_size=(3,3),\n",
    "\t\t\t\t\t strides=1, padding='same',\n",
    "\t\t\t\t\t activation='relu'))\n",
    "\tmodel.add(Conv2D(128, kernel_size=(3,3),\n",
    "\t\t\t\t\t strides=1, padding='same',\n",
    "\t\t\t\t\t activation='relu'))\n",
    "\t# model.add(MaxPooling2D(pool_size=(3,3), strides=2))\n",
    "\t# model.add(Conv2D(256, kernel_size=(3,3),\n",
    "\t# \t\t\t\t strides=1, padding='same',\n",
    "\t# \t\t\t\t activation='relu'))\n",
    "\t# model.add(Conv2D(128, kernel_size=(3,3),\n",
    "\t# \t\t\t\t strides=1, padding='same',\n",
    "\t# \t\t\t\t activation='relu'))\n",
    "\treturn model\n",
    "\n",
    "def create_conv_branch_pos(input_shape):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(96, kernel_size=(11,11),\n",
    "\t\t\t\t\t strides=4, padding='valid',\n",
    "\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\t input_shape=input_shape))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\tmodel.add(Conv2D(128, kernel_size=(7,7),\n",
    "\t\t\t\t\t strides=1, padding='same',\n",
    "\t\t\t\t\t activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(3,3), strides=1))\n",
    "\tmodel.add(Conv2D(256, kernel_size=(5,5),\n",
    "\t\t\t\t\t strides=1, padding='same',\n",
    "\t\t\t\t\t activation='relu'))\n",
    "\tmodel.add(Conv2D(256, kernel_size=(3,3),\n",
    "\t\t\t\t\t strides=1, padding='same',\n",
    "\t\t\t\t\t activation='relu'))\n",
    "\tmodel.add(Conv2D(128, kernel_size=(3,3),\n",
    "\t\t\t\t\t strides=1, padding='same',\n",
    "\t\t\t\t\t activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(3,3), strides=2))\n",
    "\tmodel.add(Conv2D(256, kernel_size=(5,5),\n",
    "\t\t\t\t\t strides=1, padding='same',\n",
    "\t\t\t\t\t activation='relu'))\n",
    "\tmodel.add(Conv2D(128, kernel_size=(3,3),\n",
    "\t\t\t\t\t strides=1, padding='same',\n",
    "\t\t\t\t\t activation='relu'))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b8fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/63827339/how-to-build-a-custom-data-generator-for-keras-tf-keras-where-x-images-are-being\n",
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    ''' Custom DataGenerator to load img \n",
    "    \n",
    "    Arguments:\n",
    "        data_frame = pandas data frame in filenames and labels format\n",
    "        batch_size = divide data in batches\n",
    "        shuffle = shuffle data before loading\n",
    "        img_shape = image shape in (h, w, d) format\n",
    "        augmentation = data augmentation to make model rebust to overfitting\n",
    "    \n",
    "    Output:\n",
    "        Img: numpy array of image\n",
    "        label : output label for image\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data_frame, labels, batch_size=10, img_shape=None, augmentation=True, num_classes=None):\n",
    "        self.data_frame = data_frame\n",
    "        self.labels = labels\n",
    "        self.train_len = len(data_frame)\n",
    "        self.batch_size = batch_size\n",
    "        self.img_shape = img_shape\n",
    "        self.num_classes = num_classes\n",
    "        print(f\"Found {self.data_frame.shape[0]} images belonging to {self.num_classes} classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        ''' return total number of batches '''\n",
    "        #self.data_frame = shuffle(self.data_frame)\n",
    "        return math.ceil(self.train_len/self.batch_size)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        ''' shuffle data after every epoch '''\n",
    "        # fix on epoch end it's not working, adding shuffle in len for alternative\n",
    "        pass\n",
    "    \n",
    "    def __data_augmentation(self, img):\n",
    "        ''' function for apply some data augmentation '''\n",
    "        img = tf.image.resize(img, 512, 640)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        return img\n",
    "        \n",
    "    def __get_image(self, file_id):\n",
    "        \"\"\" open image with file_id path and apply data augmentation \"\"\"\n",
    "        img = np.asarray(Image.open(file_id))\n",
    "        img = np.resize(img, self.img_shape)\n",
    "        img = self.__data_augmentation(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __get_label(self, label_id):\n",
    "        \"\"\" uncomment the below line to convert label into categorical format \"\"\"\n",
    "        #label_id = tf.keras.utils.to_categorical(label_id, num_classes)\n",
    "        return label_id\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # batch_x = self.data_frame[\"filenames\"][idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        # batch_y = self.data_frame[\"labels\"][idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        # # read your data here using the batch lists, batch_x and batch_y\n",
    "        # x = [self.__get_image(file_id) for file_id in batch_x] \n",
    "        # y = [self.__get_label(label_id) for label_id in batch_y]\n",
    "\n",
    "        y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        paths = self.data_frame[\"path\"][idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        base_paths = self.data_frame[\"base_path\"][idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        #raw = [tf.io.read_file(path) for path in paths]\n",
    "        image = [tf.image.resize(tf.image.decode_png(tf.io.read_file(path), channels=3), (512,640)) for path in paths]\n",
    "        #base_raw = tf.io.read_file(base_path)\n",
    "        base_image = [tf.image.resize(tf.image.decode_png(tf.io.read_file(path), channels=3), (512,640)) for path in base_paths]\n",
    "\n",
    "        return [np.asarray(base_image), np.asarray(image)], y\n",
    "\n",
    "        #return tf.convert_to_tensor(x), tf.convert_to_tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae3acaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 22:09:21.403642: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-24 22:09:21.626639: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-24 22:09:21.626957: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-24 22:09:21.629304: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-24 22:09:21.629572: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-24 22:09:21.629836: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-24 22:09:21.804451: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-24 22:09:21.804760: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-24 22:09:21.804776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-24 22:09:21.805053: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-24 22:09:21.805082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9702 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 512, 640, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 512, 640, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 30, 38, 128)          3456256   ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 30, 38, 256)          0         ['sequential[0][0]',          \n",
      "                                                                     'sequential[1][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 291840)               0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 291840)               0         ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 16)                   4669456   ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 16)                   0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 16)                   272       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 16)                   0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 16)                   272       ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 7)                    119       ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8126375 (31.00 MB)\n",
      "Trainable params: 8126375 (31.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 512, 640, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 512, 640, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 30, 38, 128)          3456256   ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 30, 38, 256)          0         ['sequential[0][0]',          \n",
      "                                                                     'sequential[1][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 291840)               0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 291840)               0         ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 16)                   4669456   ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 16)                   0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 16)                   272       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 16)                   0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 16)                   272       ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 7)                    119       ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8126375 (31.00 MB)\n",
      "Trainable params: 8126375 (31.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Found 1875 images belonging to None classes\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 22:09:42.780508: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-04-24 22:09:43.215798: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-04-24 22:09:47.484856: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.71GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-24 22:09:47.679924: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.71GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-24 22:09:51.751032: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.73GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-24 22:09:53.495569: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.73GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-24 22:09:58.092412: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.49GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-24 22:09:58.194199: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.49GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-24 22:10:00.349615: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.49GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-24 22:10:00.629924: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.49GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-24 22:10:06.096467: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.45GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-24 22:10:06.096554: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 591.66MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-04-24 22:10:08.367633: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-04-24 22:10:09.114360: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fb37a513e20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-24 22:10:09.114415: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-04-24 22:10:09.128093: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1714014609.229709    4514 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 280s 11s/step - loss: 58.7322\n",
      "Epoch 2/3\n",
      "19/19 [==============================] - 168s 9s/step - loss: 58.5057\n",
      "Epoch 3/3\n",
      "19/19 [==============================] - 161s 8s/step - loss: 58.3347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.functional.Functional at 0x7fb45807bb50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_shape=(512, 640, 3)\n",
    "\n",
    "# Rotation model\n",
    "# conv_branch = create_conv_branch(input_shape)\n",
    "# branch_a = Input(shape=input_shape)\n",
    "# branch_b = Input(shape=input_shape)\n",
    "\n",
    "# processed_a = conv_branch(branch_a)\n",
    "# processed_b = conv_branch(branch_b)\n",
    "\n",
    "# concat = keras.layers.concatenate([processed_a, processed_b])\n",
    "# x1 = Flatten()(concat)\n",
    "# output = Dense(4)(x1)\n",
    "\n",
    "# model_rot = Model(inputs=[branch_a,branch_b], outputs=output)\n",
    "# model_rot.compile(optimizer=Adam(learning_rate=0.0025), loss=\"mae\")\n",
    "# model_rot.summary()\n",
    "# load_images_and_train_model(model_rot, False)\n",
    "\n",
    "# Position model\n",
    "conv_branch = create_conv_branch_pos(input_shape)\n",
    "branch_a = Input(shape=input_shape)\n",
    "branch_b = Input(shape=input_shape)\n",
    "\n",
    "processed_a = conv_branch(branch_a)\n",
    "processed_b = conv_branch(branch_b)\n",
    "\n",
    "concat = keras.layers.concatenate([processed_a, processed_b])\n",
    "\n",
    "x1 = Flatten()(concat)\n",
    "x1 = Dropout(0.25)(x1)\n",
    "x1 = Dense(16)(x1)\n",
    "x1 = Dropout(0.25)(x1)\n",
    "x1 = Dense(16)(x1)\n",
    "x1 = Dropout(0.25)(x1)\n",
    "x1 = Dense(16)(x1)\n",
    "output = Dense(7)(x1)\n",
    "\n",
    "model_pos = Model(inputs=[branch_a,branch_b], outputs=output)\n",
    "model_pos.compile(optimizer=Adam(learning_rate=0.0001), loss=\"log_cosh\")\n",
    "model_pos.summary()\n",
    "load_images_and_train_model(model_pos, True)\n",
    "\n",
    "# yhat = model(x_test)\n",
    "# var_inv = 1 / yhat.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa44dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 23:45:46.499293: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-25 23:45:46.777884: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-25 23:45:46.778160: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-25 23:45:46.780087: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-25 23:45:46.780382: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-25 23:45:46.780724: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-25 23:45:47.049960: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-25 23:45:47.050514: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-25 23:45:47.050529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-25 23:45:47.050873: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-25 23:45:47.050902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9702 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, None, 128)         640       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 640 (2.50 KB)\n",
      "Trainable params: 640 (2.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Correspondences encode - encode to d=128\n",
    "# Stack correspondences (x^1_i, x^2_i) to x^12 in R^(nx4)\n",
    "stackedCorr = [[a[0][0], a[0][1], b[0][0], b[0][1]] for (a,b) in zip(points1, points2)]\n",
    "#print(stackedCorr)\n",
    "\n",
    "m = keras.models.Sequential([\n",
    "    Input(shape=(None, 4)),\n",
    "    Dense(128, activation=\"relu\")# Embed correspondences to d=128\n",
    "    # Create graph CNN\n",
    "    # Concat to ResNet\n",
    "])\n",
    "m.summary()\n",
    "m.compile()\n",
    "m.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32809972-8532-45a5-afa6-aef92b8e84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pose(homography_matrix):\n",
    "    # Extract rotation and translation from homography matrix\n",
    "    rot_matrix = np.zeros((3, 3))\n",
    "    rot_matrix[:, :2] = homography_matrix[:, :2]\n",
    "    rot_matrix[:, 2] = np.cross(rot_matrix[:, 0], rot_matrix[:, 1])\n",
    "    translation = homography_matrix[:, 2] / np.linalg.norm(homography_matrix[:, :2])\n",
    "\n",
    "    # Convert rotation matrix to quaternion\n",
    "    trace = np.trace(rot_matrix)\n",
    "    if trace > 0:\n",
    "        s = 0.5 / np.sqrt(trace + 1.0)\n",
    "        w = 0.25 / s\n",
    "        x = (rot_matrix[2, 1] - rot_matrix[1, 2]) * s\n",
    "        y = (rot_matrix[0, 2] - rot_matrix[2, 0]) * s\n",
    "        z = (rot_matrix[1, 0] - rot_matrix[0, 1]) * s\n",
    "    else:\n",
    "        if rot_matrix[0, 0] > rot_matrix[1, 1] and rot_matrix[0, 0] > rot_matrix[2, 2]:\n",
    "            s = 2.0 * np.sqrt(1.0 + rot_matrix[0, 0] - rot_matrix[1, 1] - rot_matrix[2, 2])\n",
    "            w = (rot_matrix[2, 1] - rot_matrix[1, 2]) / s\n",
    "            x = 0.25 * s\n",
    "            y = (rot_matrix[0, 1] + rot_matrix[1, 0]) / s\n",
    "            z = (rot_matrix[0, 2] + rot_matrix[2, 0]) / s\n",
    "        elif rot_matrix[1, 1] > rot_matrix[2, 2]:\n",
    "            s = 2.0 * np.sqrt(1.0 + rot_matrix[1, 1] - rot_matrix[0, 0] - rot_matrix[2, 2])\n",
    "            w = (rot_matrix[0, 2] - rot_matrix[2, 0]) / s\n",
    "            x = (rot_matrix[0, 1] + rot_matrix[1, 0]) / s\n",
    "            y = 0.25 * s\n",
    "            z = (rot_matrix[1, 2] + rot_matrix[2, 1]) / s\n",
    "        else:\n",
    "            s = 2.0 * np.sqrt(1.0 + rot_matrix[2, 2] - rot_matrix[0, 0] - rot_matrix[1, 1])\n",
    "            w = (rot_matrix[1, 0] - rot_matrix[0, 1]) / s\n",
    "            x = (rot_matrix[0, 2] + rot_matrix[2, 0]) / s\n",
    "            y = (rot_matrix[1, 2] + rot_matrix[2, 1]) / s\n",
    "            z = 0.25 * s\n",
    "\n",
    "    return translation, [x, y, z, w]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9131278-644b-4e4d-9286-a11c365fba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, quat = calculate_pose(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d298762f-d444-46ee-802e-aec157c76f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([620.71499515, 508.70476913,   0.92980055])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd1e7fb4-ff10-4ef1-be11-340c83569e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.00029664116316386924,\n",
       " -0.00031669406374587117,\n",
       " 0.719521014529947,\n",
       " 0.049691004139257325]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "885761d8-8c2a-4b05-b908-d36f9fe299a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_image(image_path, pose):\n",
    "    # Read the image\n",
    "    image = cv.imread(image_path)\n",
    "    \n",
    "    # Extract translation and rotation from the pose\n",
    "    translation = pose[:3]\n",
    "    quaternion = pose[3:]\n",
    "    \n",
    "    # Convert quaternion to rotation matrix\n",
    "    rotation = Rotation.from_quat(quaternion).as_matrix()\n",
    "    \n",
    "    # Apply translation\n",
    "    translation_matrix = np.eye(4)\n",
    "    translation_matrix[:3, 3] = translation\n",
    "    # Apply rotation\n",
    "    transformation_matrix = np.eye(4)\n",
    "    transformation_matrix[:3, :3] = rotation\n",
    "    \n",
    "    # Apply transformation to image\n",
    "    rows, cols, _ = image.shape\n",
    "    new_image = cv.warpPerspective(image, transformation_matrix[:3], (cols, rows))\n",
    "    \n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31d79f4d-a66d-4074-bbeb-a07af8e7af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the image\n",
    "pose = [7.64022827,-46.98920059,-42.21747971,0.98997474,-0.0941886,-0.07856753,0.07003857]\n",
    "# new_image = move_image(img1path, pose)\n",
    "\n",
    "# # Show the original and moved image\n",
    "# cv.imshow('Original Image', cv.imread(img1path))\n",
    "# cv.imshow('Moved Image', new_image)\n",
    "# cv.waitKey(0)\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8cc1d9a0-5351-4298-b5ce-005436a45845",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv.imread(img1path)\n",
    "translation = np.float32(pose[:3])\n",
    "quaternion = np.float32(pose[3:])\n",
    "rotation = Rotation.from_quat(quaternion).as_matrix().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "24c5d2d9-cf68-45a2-a8cc-d9ed9aac63a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "quaternion\n",
    "print(type(quaternion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79c514cd-2c86-4e81-8a34-cc034c17c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_to_rotation_matrix(q):\n",
    "    # Extract quaternion components\n",
    "    qw, qx, qy, qz = q\n",
    "    # Compute rotation matrix\n",
    "    R = np.array([[1 - 2*qy**2 - 2*qz**2, 2*qx*qy - 2*qz*qw, 2*qx*qz + 2*qy*qw],\n",
    "                  [2*qx*qy + 2*qz*qw, 1 - 2*qx**2 - 2*qz**2, 2*qy*qz - 2*qx*qw],\n",
    "                  [2*qx*qz - 2*qy*qw, 2*qy*qz + 2*qx*qw, 1 - 2*qx**2 - 2*qy**2]])\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f27ca246-17ae-43a2-b2f4-9a5619e7715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = quaternion_to_rotation_matrix(quaternion)\n",
    "transformation_matrix = np.identity(4)\n",
    "transformation_matrix[:3, :3] = R\n",
    "transformation_matrix[:3, 3] = translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "499b3764-00c5-4539-8d9c-b3a0a6b90743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9926d465-57f7-4c0f-a909-6e71f7483188",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /home/conda/feedstock_root/build_artifacts/libopencv_1704864625327/work/modules/imgproc/src/imgwarp.cpp:2757: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 2 && M0.cols == 3 in function 'warpAffine'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarpAffine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1280\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /home/conda/feedstock_root/build_artifacts/libopencv_1704864625327/work/modules/imgproc/src/imgwarp.cpp:2757: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 2 && M0.cols == 3 in function 'warpAffine'\n"
     ]
    }
   ],
   "source": [
    "cv.warpAffine(image, rotation, (1280, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c69b98d0-8736-4923-b50d-01726076741f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.97784348,  -0.1238725 ,  -0.16875341,   7.64022827],\n",
       "       [  0.15347316,   0.97244621,   0.17548315, -46.98920059],\n",
       "       [  0.14236607,  -0.19749418,   0.9699113 , -42.21747971],\n",
       "       [  0.        ,   0.        ,   0.        ,   1.        ]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8c81727e-05c8-4f75-a7c7-de597128b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize transformation matrix\n",
    "transformation_matrix = transformation_matrix / transformation_matrix[3, 3]\n",
    "\n",
    "# Extract rotation and translation components\n",
    "rotation_matrix = transformation_matrix[0:3, 0:3]\n",
    "translation_vector = transformation_matrix[0:3, 3]\n",
    "\n",
    "projection_matrix = np.hstack([rotation_matrix, translation_vector.reshape(3, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "be42ee80-680a-43e0-a55f-b84045e59ed8",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /home/conda/feedstock_root/build_artifacts/libopencv_1704864625327/work/modules/imgproc/src/imgwarp.cpp:3323: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 3 && M0.cols == 3 in function 'warpPerspective'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformed_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarpPerspective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojection_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /home/conda/feedstock_root/build_artifacts/libopencv_1704864625327/work/modules/imgproc/src/imgwarp.cpp:3323: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 3 && M0.cols == 3 in function 'warpPerspective'\n"
     ]
    }
   ],
   "source": [
    "transformed_image = cv.warpPerspective(image, projection_matrix, (image.shape[1], image.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a9530-b2b7-4157-8b70-dab517bd7774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e05f0608-f2bf-4e7e-ab90-0a02bc362145",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /home/conda/feedstock_root/build_artifacts/libopencv_1704864625327/work/modules/imgproc/src/imgwarp.cpp:3323: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 3 && M0.cols == 3 in function 'warpPerspective'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformed_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarpPerspective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformation_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /home/conda/feedstock_root/build_artifacts/libopencv_1704864625327/work/modules/imgproc/src/imgwarp.cpp:3323: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 3 && M0.cols == 3 in function 'warpPerspective'\n"
     ]
    }
   ],
   "source": [
    "transformed_image = cv.warpPerspective(image, transformation_matrix, (image.shape[1], image.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4b676-5caa-4a20-ae94-5153510cd6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a3d31-51d0-4c36-b48e-c3207b9fa7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "41a947b2-b119-429b-8905-7c55671d2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine translation and rotation into transformation matrix\n",
    "transformation_matrix = np.eye(4)\n",
    "transformation_matrix[:3, :3] = rotation\n",
    "transformation_matrix[:3, 3] = translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ca2ecf99-c955-4dc7-83ce-9b4fdf03dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols, _ = image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d266fef7-0c15-4a51-8563-66454596c8a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /home/conda/feedstock_root/build_artifacts/libopencv_1704864625327/work/modules/imgproc/src/imgwarp.cpp:3323: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 3 && M0.cols == 3 in function 'warpPerspective'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarpPerspective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformation_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /home/conda/feedstock_root/build_artifacts/libopencv_1704864625327/work/modules/imgproc/src/imgwarp.cpp:3323: error: (-215:Assertion failed) (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 3 && M0.cols == 3 in function 'warpPerspective'\n"
     ]
    }
   ],
   "source": [
    "new_image = cv.warpPerspective(image, transformation_matrix[:3], (cols, rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c4c688-6554-4aab-b80d-4e632fcf55ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
